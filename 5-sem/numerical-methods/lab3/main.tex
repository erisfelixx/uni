\documentclass[a4paper,12pt]{article}
\usepackage[utf8]{inputenc}
\usepackage[ukrainian]{babel}
\usepackage{geometry}
\usepackage{amsmath}
\usepackage{graphicx}
\usepackage{float}
\geometry{left=25mm,right=15mm,top=20mm,bottom=20mm}

\begin{document}

% --- титульна сторінка ---
\begin{titlepage}
    \centering
    {\large Київський національний університет імені Тараса Шевченка \par}
    {\large Факультет комп'ютерних наук та кібернетики \par}
    {\large Кафедра інтелектуальних програмних систем \par}
    {\large Чисельні методи в інформатиці \par}
    \vspace{5cm}

    {\LARGE \textbf{Звіт} \par}
    {\Large з лабораторної роботи №3 \par}
    {\Large «Наближенi методи розв’язання задач на власнi значення» \par}
    \vspace{1cm}

    {\Large Варіант №1 \par}
    \vspace{1cm}

    {\large студентки 3-го курсу \par}
    {\large групи ІПС-31 \par}
    {\Large \textbf{Совгирі Анни} \par}

    \vfill

    {\large Київ --- 2025}
\end{titlepage}

% --- Основна частина ---
\newpage

\section*{Вступ}
У даній лабораторній роботі розглядаються чисельні методи обчислення власних значень та власних векторів матриць. Зокрема, реалізуються три алгоритми: метод скалярних добутків, нормалізований метод скалярних добутків та степеневий метод. 
\par\vspace{5mm}
Метою роботи є дослідження збіжності цих методів, аналіз їхньої точності та практичне закріплення теоретичних знань щодо ітераційних процедур знаходження власних значень. У межах роботи формується квадратна матриця розміром 5×5 з цілих чисел, після чого проводиться покрокова реалізація та порівняльний аналіз отриманих результатів.

\section*{Теорія}


\subsection*{Метод скалярних добутків}

Метод скалярних добутків є наближеним ітераційним методом знаходження найбільшого за модулем власного значення матриці $A$. Нехай власні значення матриці задовольняють нерівність 
$|\lambda_{1}| > |\lambda_{2}| > \ldots > |\lambda_{n}|$. 
Тоді послідовність ітерацій наближає власний вектор, що відповідає $\lambda_{1}$, а також дозволяє обчислити саме значення $\lambda_{1}$.

Початкове наближення $\vec{x}^{\,0}$ обирається довільним, але таким, що $\vec{x}^{\,0} \neq 0$. Ітераційний процес має вигляд:
\[
\vec{x}^{\,k+1} = A \vec{x}^{\,k}.
\]

На кожній ітерації власне значення оцінюють за формулою скалярного добутку:
\[
\lambda_{1}^{k+1} = 
\frac{ \left( \vec{x}^{\,k+1}, \vec{x}^{\,k} \right) }
     { \left( \vec{x}^{\,k}, \vec{x}^{\,k} \right) }.
\]

Процес продовжують до виконання умови збіжності:
\[
|\lambda_{1}^{k+1} - \lambda_{1}^{k}| \leq \varepsilon.
\]



\subsection*{Нормалізований метод скалярних добутків}

Нормалізований метод скалярних добутків є модифікацією звичайного методу скалярних добутків, що усуває проблему неконтрольованого зростання норми векторів на ітераціях. Як і в базовому методі, вважаємо, що власні значення матриці $A$ впорядковані за модулем:
\[
|\lambda_{1}| > |\lambda_{2}| > \ldots > |\lambda_{n}|,
\]
і метою є знаходження найбільшого власного значення $\lambda_{1}$.

Початкове наближення $\vec{x}^{\,0}$ обирається довільним, але $\vec{x}^{\,0} \neq 0$. На кожній ітерації вектор попередньо нормують:
\[
\vec{e}^{\,k} = \frac{\vec{x}^{\,k}}{\|\vec{x}^{\,k}\|}.
\]
Далі обчислюють нове наближення:
\[
\vec{x}^{\,k+1} = A \vec{e}^{\,k}.
\]

Оцінка найбільшого власного значення виконується за формулою скалярного добутку:
\[
\lambda_{1}^{k+1} = 
\frac{\left( \vec{x}^{\,k+1}, \vec{e}^{\,k} \right)}
     {\left( \vec{e}^{\,k}, \vec{e}^{\,k} \right)}.
\]

Ітерації продовжують до виконання критерію збіжності:
\[
|\lambda_{1}^{k+1} - \lambda_{1}^{k}| \leq \varepsilon.
\]

Нормування на кожному кроці забезпечує чисельну стійкість методу та запобігає перенасиченню компонент вектора, завдяки чому напрямок наближення стабільно збігається до власного вектора, що відповідає $\lambda_{1}$, навіть для матриць із великими елементами чи значною різницею масштабів між власними значеннями.




\subsection*{Степеневий метод}

Нехай $|\lambda_{1}| > |\lambda_{2}| > \ldots > |\lambda_{n}|$. Будемо також шукати максимальне власне значення $\lambda_{1}$. Початкове наближення $\vec{x}^{0}$ обираємо довільним, але $\vec{x}^{0} \neq 0$.

Ітераційний процес має вигляд:
\[
\vec{x}^{\,k+1} = A \vec{x}^{\,k}; \qquad 
\lambda_{1}^{k+1} = \frac{x^{k+1}_{m}}{x^{k}_{m}}, \qquad \forall m : 1 \leq m \leq n.
\]

Умова припинення: $|\lambda_{1}^{k+1} - \lambda_{1}^{k}| \leq \varepsilon$.

\textit{Зауваження.} Якщо $A = A^{T} > 0$, то можна знайти мінімальне власне значення:
\[
\lambda_{\min}(A) = \lambda_{\max}(A) - \lambda_{\max}(B),
\]
де $B = \lambda_{\max}(A)E - A$, а $E$ – одинична матриця.


\section*{Розв’язання}

Для демонстрації роботи розглянутих алгоритмів обчислення власних значень та власних векторів задамо квадратну матрицю розмірності $5 \times 5$ з цілими елементами:
\[
A =
\begin{pmatrix}
6 & 2 & 1 & 0 & 0 \\
2 & 7 & 2 & 1 & 0 \\
1 & 2 & 8 & 2 & 1 \\
0 & 1 & 2 & 7 & 2 \\
0 & 0 & 1 & 2 & 6
\end{pmatrix}.
\]
Як початкове наближення власного вектора візьмемо вектор
\[
\vec{x}^{\,0} = (1, 1, 1, 1, 1)^{T},
\]
а точність обчислень покладемо $\varepsilon = 10^{-4}$.

\subsection*{Метод скалярних добутків}
\textbf{Ітерація 1}

\[
x^{(0)} = (1, 1, 1, 1, 1)^{T}
\]

\[
x^{(1)} = A x^{(0)} = (9, 12, 14, 12, 9)^{T}.
\]

Скалярні добутки:
\[
(x^{(1)}, x^{(0)}) = 9 + 12 + 14 + 12 + 9 = 56,
\]
\[
(x^{(0)}, x^{(0)}) = 1 + 1 + 1 + 1 + 1 = 5.
\]

Тому
\[
\lambda^{(1)} = \frac{56}{5} = 11.2.
\]

\hrulefill

\textbf{Ітерація 2}

\[
x^{(1)} = (9, 12, 14, 12, 9)^{T}
\]

\[
x^{(2)} = A x^{(1)} = (92, 142, 178, 142, 92)^{T}.
\]

\[
(x^{(2)}, x^{(1)}) = 
92 \cdot 9 + 142 \cdot 12 + 178 \cdot 14 + 142 \cdot 12 + 92 \cdot 9 = 7556,
\]
\[
(x^{(1)}, x^{(1)}) = 
9^{2} + 12^{2} + 14^{2} + 12^{2} + 9^{2} = 646.
\]

\[
\lambda^{(2)} = \frac{7556}{646} \approx 11.6965944272.
\]

Різниця:
\[
|\lambda^{(2)} - \lambda^{(1)}| \approx 0.4966 > \varepsilon \Rightarrow \text{продовжуємо}.
\]

\hrulefill

\textbf{Ітерація 3}

\[
x^{(2)} = (92, 142, 178, 142, 92)^{T}
\]

\[
x^{(3)} = A x^{(2)} = (1014, 1676, 2176, 1676, 1014)^{T}.
\]

\[
(x^{(3)}, x^{(2)}) =
1014\cdot92 + 1676\cdot142 + 2176\cdot178 + 1676\cdot142 + 1014\cdot92 = 1\,049\,888,
\]
\[
(x^{(2)}, x^{(2)}) =
92^{2} + 142^{2} + 178^{2} + 142^{2} + 92^{2} = 88\,940.
\]

\[
\lambda^{(3)} =
\frac{1\,049\,888}{88\,940}
\approx 11.8044524398.
\]

\[
|\lambda^{(3)} - \lambda^{(2)}| \approx 0.1079 > \varepsilon.
\]

\hrulefill

\textbf{Ітерація 4}

\[
x^{(3)} = (1014, 1676, 2176, 1676, 1014)^{T}
\]

\[
x^{(4)} = A x^{(3)} = (11612, 19788, 26140, 19788, 11612)^{T}.
\]

\[
(x^{(4)}, x^{(3)}) = 146\,759\,152,
\qquad
(x^{(3)}, x^{(3)}) = 12\,409\,320.
\]

\[
\lambda^{(4)} =
\frac{146\,759\,152}{12\,409\,320}
\approx 11.8265265139.
\]

\[
|\lambda^{(4)} - \lambda^{(3)}| \approx 0.02207 > \varepsilon.
\]

\hrulefill

\textbf{Ітерація 5}

\[
x^{(4)} = (11612, 19788, 26140, 19788, 11612)^{T}
\]

\[
x^{(5)} = A x^{(4)} = (135388, 233808, 311496, 233808, 135388)^{T}.
\]

\[
(x^{(5)}, x^{(4)}) = 20\,539\,941\,760,
\qquad
(x^{(4)}, x^{(4)}) = 1\,736\,106\,576.
\]

\[
\lambda^{(5)} =
\frac{20\,539\,941\,760}{1\,736\,106\,576}
\approx 11.8310373591.
\]

\[
|\lambda^{(5)} - \lambda^{(4)}| \approx 0.00451 > \varepsilon.
\]

\hrulefill

\textbf{Ітерація 6}

\[
x^{(5)} = (135388, 233808, 311496, 233808, 135388)^{T}
\]

\[
x^{(6)} = A x^{(5)} = (1\,591\,440, 2\,764\,232, 3\,697\,976, 2\,764\,232, 1\,591\,440)^{T}.
\]

\[
(x^{(6)}, x^{(5)}) = 2\,875\,427\,600\,448,
\qquad
(x^{(5)}, x^{(5)}) = 243\,021\,940\,832.
\]

\[
\lambda^{(6)} =
\frac{2\,875\,427\,600\,448}{243\,021\,940\,832}
\approx 11.8319670669.
\]

\[
|\lambda^{(6)} - \lambda^{(5)}| \approx 0.00093 > \varepsilon.
\]

\hrulefill

\textbf{Ітерація 7}

\[
x^{(6)} =
(1\,591\,440, 2\,764\,232, 3\,697\,976, 2\,764\,232, 1\,591\,440)^{T}
\]

\[
x^{(7)} = A x^{(6)} =
(18\,775\,080, 32\,692\,688, 43\,823\,616, 32\,692\,688, 18\,775\,080)^{T}.
\]

\[
(x^{(7)}, x^{(6)}) = 402\,557\,855\,502\,848,
\qquad
(x^{(6)}, x^{(6)}) = 34\,022\,346\,143\,424.
\]

\[
\lambda^{(7)} =
\frac{402\,557\,855\,502\,848}{34\,022\,346\,143\,424}
\approx 11.8321603632.
\]

\[
|\lambda^{(7)} - \lambda^{(6)}| \approx 0.0001933 > \varepsilon.
\]

\hrulefill

\textbf{Ітерація 8}

\[
x^{(7)} =
(18\,775\,080, 32\,692\,688, 43\,823\,616, 32\,692\,688, 18\,775\,080)^{T}
\]

\[
x^{(8)} =
A x^{(7)} =
(221\,859\,472,\; 386\,738\,896,\; 518\,909\,840,\; 386\,738\,896,\; 221\,859\,472)^{T}.
\]

\[
(x^{(8)}, x^{(7)}) =
56\,358\,432\,366\,681\,856,
\qquad
(x^{(7)}, x^{(7)}) =
4\,763\,140\,274\,658\,944.
\]

\[
\lambda^{(8)} =
\frac{56\,358\,432\,366\,681\,856}
     {4\,763\,140\,274\,658\,944}
\approx 11.8322008416.
\]

Різниця:
\[
|\lambda^{(8)} - \lambda^{(7)}| \approx 4.05\cdot10^{-5} < \varepsilon.
\]

\[
\text{умова зупинки виконується} \Rightarrow
\text{метод зійшовся за 8 ітерацій}.
\]
\textbf{Наш розв'язок збігається з результатом програмної реалізації.}


\par\vspace{5mm}
\subsection*{Нормалізований метод скалярних добутків}

\textbf{Ітерація 1}

\[
x^{(0)} = (1, 1, 1, 1, 1)^{T}.
\]

Нормування:
\[
\|x^{(0)}\| = \sqrt{5} \approx 2{,}2361, \qquad
e^{(0)} = \frac{x^{(0)}}{\|x^{(0)}\|}
\approx (0{,}4472,\; 0{,}4472,\; 0{,}4472,\; 0{,}4472,\; 0{,}4472)^{T}.
\]

Новий вектор:
\[
x^{(1)} = A e^{(0)} \approx (4{,}0249,\; 5{,}3666,\; 6{,}2610,\; 5{,}3666,\; 4{,}0249)^{T}.
\]

Оцінка власного значення:
\[
\lambda^{(1)} = (x^{(1)}, e^{(0)}) \approx 11{,}2.
\]

\hrulefill

\textbf{Ітерація 2}

\[
x^{(1)} \approx (4{,}0249,\; 5{,}3666,\; 6{,}2610,\; 5{,}3666,\; 4{,}0249)^{T},
\]
\[
e^{(1)} = \frac{x^{(1)}}{\|x^{(1)}\|}
\approx (0{,}3541,\; 0{,}4721,\; 0{,}5508,\; 0{,}4721,\; 0{,}3541)^{T},
\]
\[
x^{(2)} = A e^{(1)} \approx (3{,}6197,\; 5{,}5869,\; 7{,}0033,\; 5{,}5869,\; 3{,}6197)^{T},
\]
\[
\lambda^{(2)} = (x^{(2)}, e^{(1)}) \approx 11{,}6966.
\]

Різниця:
\[
|\lambda^{(2)} - \lambda^{(1)}|
\approx 0{,}4966 > \varepsilon \Rightarrow \text{продовжуємо.}
\]

\hrulefill

\textbf{Ітерація 3}

\[
x^{(2)} \approx (3{,}6197,\; 5{,}5869,\; 7{,}0033,\; 5{,}5869,\; 3{,}6197)^{T},
\]
\[
e^{(2)} = \frac{x^{(2)}}{\|x^{(2)}\|}
\approx (0{,}3085,\; 0{,}4761,\; 0{,}5969,\; 0{,}4761,\; 0{,}3085)^{T},
\]
\[
x^{(3)} = A e^{(2)} \approx (3{,}4001,\; 5{,}6199,\; 7{,}2964,\; 5{,}6199,\; 3{,}4001)^{T},
\]
\[
\lambda^{(3)} = (x^{(3)}, e^{(2)}) \approx 11{,}8045.
\]

\[
|\lambda^{(3)} - \lambda^{(2)}|
\approx 0{,}1079 > \varepsilon.
\]

\hrulefill

\textbf{Ітерація 4}

\[
x^{(3)} \approx (3{,}4001,\; 5{,}6199,\; 7{,}2964,\; 5{,}6199,\; 3{,}4001)^{T},
\]
\[
e^{(3)} = \frac{x^{(3)}}{\|x^{(3)}\|}
\approx (0{,}2878,\; 0{,}4758,\; 0{,}6177,\; 0{,}4758,\; 0{,}2878)^{T},
\]
\[
x^{(4)} = A e^{(3)} \approx (3{,}2963,\; 5{,}6173,\; 7{,}4205,\; 5{,}6173,\; 3{,}2963)^{T},
\]
\[
\lambda^{(4)} = (x^{(4)}, e^{(3)}) \approx 11{,}8265.
\]

\[
|\lambda^{(4)} - \lambda^{(3)}|
\approx 0{,}02207 > \varepsilon.
\]

\hrulefill

\textbf{Ітерація 5}

\[
x^{(4)} \approx (3{,}2963,\; 5{,}6173,\; 7{,}4205,\; 5{,}6173,\; 3{,}2963)^{T},
\]
\[
e^{(4)} = \frac{x^{(4)}}{\|x^{(4)}\|}
\approx (0{,}2787,\; 0{,}4749,\; 0{,}6274,\; 0{,}4749,\; 0{,}2787)^{T},
\]
\[
x^{(5)} = A e^{(4)} \approx (3{,}2493,\; 5{,}6114,\; 7{,}4759,\; 5{,}6114,\; 3{,}2493)^{T},
\]
\[
\lambda^{(5)} = (x^{(5)}, e^{(4)}) \approx 11{,}8310.
\]

\[
|\lambda^{(5)} - \lambda^{(4)}|
\approx 0{,}00451 > \varepsilon.
\]

\hrulefill

\textbf{Ітерація 6}

\[
x^{(5)} \approx (3{,}2493,\; 5{,}6114,\; 7{,}4759,\; 5{,}6114,\; 3{,}2493)^{T},
\]
\[
e^{(5)} = \frac{x^{(5)}}{\|x^{(5)}\|}
\approx (0{,}2746,\; 0{,}4743,\; 0{,}6319,\; 0{,}4743,\; 0{,}2746)^{T},
\]
\[
x^{(6)} = A e^{(5)} \approx (3{,}2283,\; 5{,}6073,\; 7{,}5014,\; 5{,}6073,\; 3{,}2283)^{T},
\]
\[
\lambda^{(6)} = (x^{(6)}, e^{(5)}) \approx 11{,}8320.
\]

\[
|\lambda^{(6)} - \lambda^{(5)}|
\approx 0{,}00093 > \varepsilon.
\]

\hrulefill

\textbf{Ітерація 7}

\[
x^{(6)} \approx (3{,}2283,\; 5{,}6073,\; 7{,}5014,\; 5{,}6073,\; 3{,}2283)^{T},
\]
\[
e^{(6)} = \frac{x^{(6)}}{\|x^{(6)}\|}
\approx (0{,}2728,\; 0{,}4739,\; 0{,}6340,\; 0{,}4739,\; 0{,}2728)^{T},
\]
\[
x^{(7)} = A e^{(6)} \approx (3{,}2188,\; 5{,}6049,\; 7{,}5132,\; 5{,}6049,\; 3{,}2188)^{T},
\]
\[
\lambda^{(7)} = (x^{(7)}, e^{(6)}) \approx 11{,}8322.
\]

\[
|\lambda^{(7)} - \lambda^{(6)}|
\approx 0{,}000193 > \varepsilon.
\]

\hrulefill

\textbf{Ітерація 8}

\[
x^{(7)} \approx (3{,}2188,\; 5{,}6049,\; 7{,}5132,\; 5{,}6049,\; 3{,}2188)^{T},
\]
\[
e^{(7)} = \frac{x^{(7)}}{\|x^{(7)}\|}
\approx (0{,}2720,\; 0{,}4737,\; 0{,}6350,\; 0{,}4737,\; 0{,}2720)^{T},
\]
\[
x^{(8)} = A e^{(7)} \approx (3{,}2146,\; 5{,}6037,\; 7{,}5187,\; 5{,}6037,\; 3{,}2146)^{T},
\]
\[
\lambda^{(8)} = (x^{(8)}, e^{(7)}) \approx 11{,}8322.
\]

Різниця:
\[
|\lambda^{(8)} - \lambda^{(7)}|
\approx 4{,}05 \cdot 10^{-5} < \varepsilon,
\]
отже, умова зупинки виконується, і нормалізований метод скалярних добутків
\textbf{збіжний за 8 ітерацій, що також підтверджує програмна реалізація}.


\subsection*{Степеневий метод}

Перед застосуванням степеневого методу перевіримо умову:
\[
A = A^{T} > 0.
\]

\textbf{1. Перевірка симетричності.}

Запишемо транспоновану матрицю:
\[
A^{T} =
\begin{pmatrix}
6 & 2 & 1 & 0 & 0 \\
2 & 7 & 2 & 1 & 0 \\
1 & 2 & 8 & 2 & 1 \\
0 & 1 & 2 & 7 & 2 \\
0 & 0 & 1 & 2 & 6
\end{pmatrix}.
\]

Оскільки $A^{T} = A$, матриця є симетричною.

\textbf{2. Перевірка додатної визначеності (головні мінори).}

Знаходимо послідовно головні мінори матриці $A$:
\[
|A_{1}| = 
\begin{vmatrix}
6
\end{vmatrix}
= 6 > 0,
\]
\[
|A_{2}| =
\begin{vmatrix}
6 & 2 \\
2 & 7
\end{vmatrix}
= 6\cdot7 - 2\cdot2 = 38 > 0,
\]
\[
|A_{3}| =
\begin{vmatrix}
6 & 2 & 1 \\
2 & 7 & 2 \\
1 & 2 & 8
\end{vmatrix}
= 281 > 0,
\]
\[
|A_{4}| =
\begin{vmatrix}
6 & 2 & 1 & 0 \\
2 & 7 & 2 & 1 \\
1 & 2 & 8 & 2 \\
0 & 1 & 2 & 7
\end{vmatrix}
= 1808 > 0,
\]
\[
|A_{5}| =
\begin{vmatrix}
6 & 2 & 1 & 0 & 0 \\
2 & 7 & 2 & 1 & 0 \\
1 & 2 & 8 & 2 & 1 \\
0 & 1 & 2 & 7 & 2 \\
0 & 0 & 1 & 2 & 6
\end{vmatrix}
= 9728 > 0.
\]

Усі головні мінори додатні, отже матриця $A$ є додатно визначеною. Таким чином умови застосування степеневого методу виконуються.

\vspace{3mm}

\textbf{Ітерація 1}

\textit{Зауважимо, що при подальших обчисленнях нумерація компонент векторів проводитиметься, 
як і в програмній реалізації, починаючи з нуля.} 
Позиція з індексом $0$ відповідає першій компоненті вектора, індекс $1$ — другій і так далі. 


\[
x^{(0)} = (1, 1, 1, 1, 1)^{T},
\]
\[
y^{(1)} = A x^{(0)} = (9, 12, 14, 12, 9)^{T}.
\]

Найбільша за модулем компонента $y^{(1)}$ — друга, тому беремо $m = 2$ і
\[
\lambda_{1}^{(1)} = \frac{y^{(1)}_{2}}{x^{(0)}_{2}} = \frac{14}{1} = 14.
\]

Наступний вектор:
\[
x^{(1)} = \frac{1}{\lambda_{1}^{(1)}}\, y^{(1)} =
\left(
\frac{9}{14},\;
\frac{12}{14},\;
1,\;
\frac{12}{14},\;
\frac{9}{14}
\right)^{T}
\approx
(0{,}6429,\; 0{,}8571,\; 1,\; 0{,}8571,\; 0{,}6429)^{T}.
\]

\textbf{Ітерація 2}

\[
x^{(1)} \approx (0{,}6429,\; 0{,}8571,\; 1,\; 0{,}8571,\; 0{,}6429)^{T},
\]
\[
y^{(2)} = A x^{(1)} \approx
(6{,}5714,\; 10{,}1429,\; 12{,}7143,\; 10{,}1429,\; 6{,}5714)^{T}.
\]

Знову найбільша компонента~— друга, тому
\[
\lambda_{1}^{(2)} = \frac{y^{(2)}_{2}}{x^{(1)}_{2}} =
\frac{12{,}7143}{1} \approx 12{,}7143.
\]

\[
x^{(2)} = \frac{1}{\lambda_{1}^{(2)}}\, y^{(2)} \approx
(0{,}5169,\; 0{,}7978,\; 1,\; 0{,}7978,\; 0{,}5169)^{T}.
\]

Різниця між послідовними наближеннями власного значення:
\[
|\lambda_{1}^{(2)} - \lambda_{1}^{(1)}|
\approx 1{,}2857 > \varepsilon,
\]
тому ітераційний процес продовжуємо.

\textbf{Ітерація 3}

\[
x^{(2)} \approx (0{,}5169,\; 0{,}7978,\; 1,\; 0{,}7978,\; 0{,}5169)^{T},
\]
\[
y^{(3)} = A x^{(2)} \approx
(5{,}6966,\; 9{,}4157,\; 12{,}2247,\; 9{,}4157,\; 5{,}6966)^{T}.
\]

Найбільша компонента знову друга, тому
\[
\lambda_{1}^{(3)} = \frac{y^{(3)}_{2}}{x^{(2)}_{2}} =
\frac{12{,}2247}{1} \approx 12{,}2247.
\]

\[
x^{(3)} = \frac{1}{\lambda_{1}^{(3)}}\, y^{(3)} \approx
(0{,}46599,\; 0{,}77022,\; 1,\; 0{,}77022,\; 0{,}46599)^{T}.
\]

\[
|\lambda_{1}^{(3)} - \lambda_{1}^{(2)}|
\approx 0{,}4896 > \varepsilon.
\]

Подальші ітерації виконуються аналогічно. За результатами програмної реалізації степеневий метод збігається до найбільшого власного значення матриці $A$ з заданою точністю $\varepsilon$ за \textbf{15 ітерацій}. 

\begin{figure}[h!]
    \centering
    \includegraphics[width=0.8\textwidth]{pic0.png}
\end{figure}

\begin{figure}[h!]
    \centering
    \includegraphics[width=0.8\textwidth]{pic1.png}
\end{figure}

\begin{figure}[h!]
    \centering
    \includegraphics[width=0.8\textwidth]{pic2.png}
    \caption{Фінальні ітерації степеневого методу}
    \label{fig:my_image}
\end{figure}



\section*{Висновок}

У роботі було реалізовано та порівняно три ітераційні методи обчислення власних значень матриці: метод скалярних добутків, його нормалізований варіант і степеневий метод. 

Усі алгоритми продемонстрували збіжність до найбільшого власного значення, однак їх поведінка в процесі ітерацій суттєво відрізняється. Звичайний метод скалярних добутків є простим у реалізації, проте норми проміжних векторів швидко зростають, що впливає на чисельну стабільність. Нормалізований метод усуває цю проблему за рахунок нормування, завдяки чому забезпечує більш передбачувану збіжність та стабільніші оцінки власного значення. Степеневий метод показав надійну роботу за умов симетричності й додатної визначеності матриці, але потребує додаткових перевірок. Загалом, отримані результати підтверджують коректність реалізацій і демонструють переваги методів, що включають нормування на кожному кроці.

\end{document}
